{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad17843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6398f48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169868\n",
      "134967\n"
     ]
    }
   ],
   "source": [
    "fake_paths=glob('d:/AI_term_project/face_crop/fake/*.jpg')\n",
    "real_paths=glob('d:/AI_term_project/face_crop/real1/*.jpg')\n",
    "\n",
    "print(len(fake_paths))\n",
    "print(len(real_paths))\n",
    "\n",
    "all_paths=fake_paths+real_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b02fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Mask\n",
    "import cv2\n",
    "from imutils import face_utils\n",
    "import dlib\n",
    "detector = dlib.get_frontal_face_detector ()\n",
    "predictor = dlib.shape_predictor('d:/AI_term_project/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def blind_face(img):\n",
    "    seed = random.random ()\n",
    "    probability = 0.5\n",
    "\n",
    "    if (seed < probability):\n",
    "        seed = random.random ()\n",
    "        case = int (seed * 3)\n",
    "        gray = cv2.cvtColor (img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        results = detector(gray, 0)\n",
    "        if len(results) > 0 :\n",
    "            shape = predictor (gray, results[0])\n",
    "            shape = face_utils.shape_to_np (shape)\n",
    "            pts_ = get_poly (shape, case, img.shape[1], img.shape[0])\n",
    "            cv2.fillConvexPoly (img, pts_, (0, 0, 0))\n",
    "            del shape, pts_\n",
    "\n",
    "        \n",
    "def get_poly(points, case_, x, y):\n",
    "    input_shape = (x, y)\n",
    "\n",
    "    pts = np.array ([points[36], points[20], points[23], points[45]], np.int32)\n",
    "\n",
    "    if case_ == 0:  # both eye\n",
    "        pts = np.array ([[points[36][0]-10, points[36][1]-20], [points[36][0]-10, points[36][1]+20], [points[45][0]+10, points[45][1]+20], [points[45][0]+10, points[45][1]-20]], np.int32)\n",
    "    elif case_ == 1:  # nose\n",
    "        pt1 = np.array ([points[31][0], points[28][1]])\n",
    "        pt2 = np.array ([points[35][0], points[28][1]])\n",
    "        pts = np.array ([points[27], pt1, points[31], points[51], points[35], pt2], np.int32)\n",
    "    elif case_ == 2:  # forehead\n",
    "        pt1 = np.array ([0, points[17][1]])\n",
    "        pt2 = np.array ([input_shape[0] - 1, points[26][1]])\n",
    "        pt3 = np.array ([input_shape[0] - 1, 0])\n",
    "        pt4 = np.array ([0, 0])\n",
    "        pts = np.array ([pt1, pt2, pt3, pt4], np.int32)\n",
    "    \n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd84f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "labeling={'fake':1, 'real':0, 'test':2}\n",
    "\n",
    "def load_img_label(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    label = labeling[path.split('/')[-1][:4]]\n",
    "    return image, label\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, X, y, batch_size, step_per_epoch, input_shape, shuffle=True, augment = True):\n",
    "        self.X = X\n",
    "        self.y = y if y is not None else None\n",
    "        self.batch_size = batch_size\n",
    "        self.step_per_epoch = step_per_epoch\n",
    "        self.input_shape = input_shape\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.augment_size = int(self.batch_size*0.3)\n",
    "        self.init_generator()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def _shuffle_sample(self):\n",
    "        if (self.shuffle == True):\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def init_generator(self):\n",
    "        self.sample_size = len(self.X)\n",
    "        self.indexes = np.zeros(self.sample_size, dtype=np.int)\n",
    "\n",
    "        for i in range(self.sample_size):\n",
    "            self.indexes[i] = int(i)\n",
    "\n",
    "        self._shuffle_sample()\n",
    "\n",
    "        self.miniEpoch_size = self.batch_size * self.step_per_epoch\n",
    "        self.cnt_mini_epoch = self.sample_size // self.miniEpoch_size\n",
    "        self.iter_mini_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if (self.iter_mini_epoch < self.cnt_mini_epoch):\n",
    "            self.mini_indexes = self.indexes[self.iter_mini_epoch * self.miniEpoch_size: (self.iter_mini_epoch + 1) * self.miniEpoch_size]\n",
    "            self.iter_mini_epoch = self.iter_mini_epoch + 1\n",
    "        else:\n",
    "            self.iter_mini_epoch = 0\n",
    "            self._shuffle_sample()\n",
    "            self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.step_per_epoch)\n",
    "\n",
    "    def __data_generation(self, X_list, y_list):\n",
    "        X = np.zeros ((self.batch_size, self.input_shape[0], self.input_shape[1], self.input_shape[2]), dtype=np.float32)\n",
    "        y = np.zeros ((self.batch_size,), dtype=int)\n",
    "\n",
    "        if y_list is not None:\n",
    "            for i, (path, label) in enumerate(zip(X_list, y_list)):\n",
    "                img, label = load_img_label(path)\n",
    "                if self.augment and i < self.augment_size:\n",
    "                    img = augmentor(img)\n",
    "                X[i] = tf.image.resize(tf.cast(img, tf.float32), (380,380)) / 255.0\n",
    "                y[i] = label\n",
    "\n",
    "            return X, y\n",
    "        else:\n",
    "            for i, path in enumerate (X_list):\n",
    "                img, _ = load_img_label(path)\n",
    "                X[i] = tf.image.resize(tf.cast(img, tf.float32), (380,380)) / 255.0\n",
    "\n",
    "            return X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.mini_indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        X_list = [self.X[k] for k in indexes]\n",
    "\n",
    "        if self.y is not None:\n",
    "            y_list = [self.y[k] for k in indexes]\n",
    "            X, y = self.__data_generation (X_list, y_list)\n",
    "            return X, y\n",
    "        else:\n",
    "            y_list = None\n",
    "            X = self.__data_generation (X_list, y_list)\n",
    "            return X\n",
    "\n",
    "def augmentor(img):\n",
    "    image = np.array(img)\n",
    "    blind_face(image)\n",
    "    transform = A.Compose([\n",
    "            A.augmentations.transforms.HorizontalFlip(p=0.5),\n",
    "            A.augmentations.transforms.GaussianBlur(p=0.3),\n",
    "            A.augmentations.transforms.GaussNoise(p=0.3),\n",
    "            A.augmentations.transforms.ImageCompression(quality_lower=60, quality_upper=100, p=0.3),\n",
    "    ])\n",
    "    transformed = transform(image=image)\n",
    "    image = transformed[\"image\"]\n",
    "    return image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97fbd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sclab\\AppData\\Local\\Temp\\ipykernel_5100\\3494557681.py:33: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.indexes = np.zeros(self.sample_size, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "train_data = DataGenerator(all_paths, all_paths, 6, 50805, (380, 380, 3), True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1568536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 380, 380, 3)]     0         \n",
      "                                                                 \n",
      " efficientnet-b4 (Functional  (None, 12, 12, 1792)     17673816  \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1792)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               918016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,592,345\n",
      "Trainable params: 18,467,145\n",
      "Non-trainable params: 125,200\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\test_v2\\lib\\site-packages\\tensorflow_addons\\optimizers\\rectified_adam.py:120: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import efficientnet.tfkeras as efficientnet\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "def model():\n",
    "    pretrained_model = efficientnet.EfficientNetB4(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(380, 380, 3),\n",
    "      )\n",
    "    pretrained_model.trainable = True\n",
    "  \n",
    "    inputs = keras.layers.Input(shape=(380, 380, 3))\n",
    "    x = pretrained_model(inputs)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.models.Model(inputs = inputs, outputs = output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tfa.optimizers.RectifiedAdam(lr=1e-5, weight_decay=0.0005), \n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b454092",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('d:/AI_term_project/model1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2896131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "filepath = 'd:/AI_term_project/380380.hdf5'\n",
    "\n",
    "mcp = callbacks.ModelCheckpoint(filepath, monitor = 'loss', save_best_only=False, save_weights_only=True, mode='min', save_freq=50, verbose=0)\n",
    "lrp = callbacks.ReduceLROnPlateau(monitor = 'loss', factor=0.25, patience=2, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa21a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   40/50805 [..............................] - ETA: 6:52:01 - loss: 8.6820 - accuracy: 0.4917"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, epochs= 5, verbose=1, callbacks=[mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4bd5a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sclab\\AppData\\Local\\Temp\\ipykernel_18388\\173843326.py:33: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.indexes = np.zeros(self.sample_size, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "test_path = 'd:/AI_term_project/test/test_crop/*.jpg'\n",
    "\n",
    "# collect all images\n",
    "images = glob(test_path)\n",
    "images.sort()\n",
    "print(len(images))\n",
    "test_data = DataGenerator(images, None, 1, 4100, (224, 224, 3), False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a9be67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('d:/AI_term_project/rectifiedAdam_90.5%.hdf5')\n",
    "p = model.predict(test_data)\n",
    "model.load_weights('d:/AI_term_project/89%.hdf5')\n",
    "p2 = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94b56ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2117\n",
      "1    1983\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dir = 'd:/AI_term_project/submission.csv'\n",
    "sc = open(dir, 'w')\n",
    "sc.write('path,y')\n",
    "sc.write('\\n')\n",
    "\n",
    "for i in range(len(images)):\n",
    "    if p[i]+p2[i] > 1.0:\n",
    "        saveline = 'leaderboard/'+images[i][-15:] + ',1'\n",
    "        sc.write(saveline)\n",
    "        sc.write('\\n')\n",
    "    else:\n",
    "        saveline = 'leaderboard/'+images[i][-15:] + ',0'\n",
    "        sc.write(saveline)\n",
    "        sc.write('\\n')\n",
    "\n",
    "sc.close()\n",
    "\n",
    "submission = pd.read_csv(dir, index_col = False)\n",
    "print(submission[\"y\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74fb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
